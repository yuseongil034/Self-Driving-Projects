# ğŸš— ADAS ì™„ë²½ ê°€ì´ë“œ: TensorRT vs PyTorch
### ì²¨ë‹¨ ìš´ì „ì ë³´ì¡° ì‹œìŠ¤í…œì˜ ëª¨ë“  ê²ƒ

[![ADAS](https://img.shields.io/badge/ADAS-Advanced_Driver_Assistance-blue?style=for-the-badge)](https://en.wikipedia.org/wiki/Advanced_driver-assistance_systems)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch)](https://pytorch.org/)
[![TensorRT](https://img.shields.io/badge/TensorRT-8.6+-76B900?style=for-the-badge&logo=nvidia)](https://developer.nvidia.com/tensorrt)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)

> ğŸ¯ **ì´ ê°€ì´ë“œì˜ ëª©í‘œ**: ADAS ê¸°ìˆ ì„ ì´í•´í•˜ê³ , PyTorchë¡œ ëª¨ë¸ì„ ê°œë°œí•œ í›„ TensorRTë¡œ ìµœì í™”í•˜ì—¬ ì‹¤ì œ ì°¨ëŸ‰ì— ë°°í¬í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë°°ì›ë‹ˆë‹¤.

---

## ğŸ“š ëª©ì°¨

1. [ADASë€ ë¬´ì—‡ì¸ê°€?](#-adasë€-ë¬´ì—‡ì¸ê°€)
2. [ADAS ì£¼ìš” ê¸°ëŠ¥ ìƒì„¸ ì„¤ëª…](#-adas-ì£¼ìš”-ê¸°ëŠ¥-ìƒì„¸-ì„¤ëª…)
3. [ADAS ëª¨ë¸ ê°œë°œ íë¦„](#-adas-ëª¨ë¸-ê°œë°œ-íë¦„)
4. [TensorRT vs PyTorch ì™„ë²½ ë¹„êµ](#-tensorrt-vs-pytorch-ì™„ë²½-ë¹„êµ)
5. [ì‹¤ìŠµ: PyTorchì—ì„œ TensorRTë¡œ](#-ì‹¤ìŠµ-pytorchì—ì„œ-tensorrtë¡œ)
6. [ì„±ëŠ¥ ìµœì í™” íŒ](#-ì„±ëŠ¥-ìµœì í™”-íŒ)
7. [ì‹¤ì œ ì°¨ëŸ‰ ë°°í¬ ê°€ì´ë“œ](#-ì‹¤ì œ-ì°¨ëŸ‰-ë°°í¬-ê°€ì´ë“œ)
8. [ìš©ì–´ì§‘](#-ìš©ì–´ì§‘)
9. [ì¶”ê°€ í•™ìŠµ ìë£Œ](#-ì¶”ê°€-í•™ìŠµ-ìë£Œ)

---

## ğŸš˜ ADASë€ ë¬´ì—‡ì¸ê°€?

### ğŸ“– ì‰¬ìš´ ì •ì˜

**ADAS (Advanced Driver Assistance Systems)**ëŠ” ìš´ì „ì„ ë” ì•ˆì „í•˜ê³  í¸ë¦¬í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” "ìŠ¤ë§ˆíŠ¸ ë„ìš°ë¯¸"ì…ë‹ˆë‹¤.

**ì¼ìƒ ë¹„ìœ **: 
- ADASëŠ” ìš´ì „í•  ë•Œ ì˜†ì—ì„œ ë„ì™€ì£¼ëŠ” **"ê²½í—˜ ë§ì€ ì¡°ìˆ˜ì„ ì¹œêµ¬"** ê°™ìŠµë‹ˆë‹¤
- ìœ„í—˜ì„ ë¯¸ë¦¬ ì•Œë ¤ì£¼ê³  âš ï¸
- ì‹¤ìˆ˜ë¥¼ ë°”ë¡œì¡ì•„ì£¼ê³  âœ…
- ê¸´ ìš´ì „ì—ì„œ í”¼ë¡œë¥¼ ëœì–´ì¤ë‹ˆë‹¤ ğŸ˜Š

### ğŸ¯ ADASì˜ í•µì‹¬ ëª©í‘œ

```mermaid
graph LR
    A[ì‚¬ê³  ì˜ˆë°©] --> D[ADAS]
    B[ìš´ì „ í¸ì˜] --> D
    C[í”¼ë¡œ ê°ì†Œ] --> D
    D --> E[ì•ˆì „í•œ ë„ë¡œ]
```

### ğŸ”„ ADAS ì‘ë™ ì›ë¦¬

```python
# ADAS ì‹œìŠ¤í…œì˜ ê¸°ë³¸ íë¦„
class ADASSystem:
    def __init__(self):
        self.sensors = ["ì¹´ë©”ë¼", "ë ˆì´ë”", "ë¼ì´ë‹¤", "ì´ˆìŒíŒŒ"]
        self.processors = ["AI ëª¨ë¸", "ì‹ í˜¸ ì²˜ë¦¬"]
        self.actuators = ["ë¸Œë ˆì´í¬", "ì¡°í–¥", "ê°€ì†"]
    
    def process(self):
        """ADAS ì²˜ë¦¬ ê³¼ì •"""
        # 1. ê°ì§€: ì£¼ë³€ í™˜ê²½ ì¸ì‹
        environment = self.sense_environment()
        
        # 2. íŒë‹¨: AIê°€ ìƒí™© ë¶„ì„
        decision = self.analyze_situation(environment)
        
        # 3. í–‰ë™: ì°¨ëŸ‰ ì œì–´
        self.execute_action(decision)
        
        return "ì•ˆì „ ìš´ì „ ë³´ì¡° ì¤‘ âœ…"
```

---

## ğŸ›¡ï¸ ADAS ì£¼ìš” ê¸°ëŠ¥ ìƒì„¸ ì„¤ëª…

### 1. ğŸš¦ ACC (Adaptive Cruise Control) - ì ì‘í˜• í¬ë£¨ì¦ˆ ì»¨íŠ¸ë¡¤

**ì‘ë™ ì›ë¦¬**: ì•ì°¨ì™€ì˜ ê±°ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ìœ ì§€í•˜ë©° ì†ë„ ì¡°ì ˆ

```python
def adaptive_cruise_control(current_speed, front_car_distance, set_speed):
    """ACC ì‹œìŠ¤í…œ ë¡œì§"""
    SAFE_DISTANCE = 50  # ë¯¸í„°
    
    if front_car_distance < SAFE_DISTANCE:
        # ì•ì°¨ê°€ ê°€ê¹Œìš°ë©´ ê°ì†
        return "ê°ì† ğŸ”½"
    elif current_speed < set_speed and front_car_distance > SAFE_DISTANCE:
        # ì•ˆì „ê±°ë¦¬ í™•ë³´ & ì„¤ì • ì†ë„ë³´ë‹¤ ëŠë¦¬ë©´ ê°€ì†
        return "ê°€ì† ğŸ”¼"
    else:
        return "ì†ë„ ìœ ì§€ â¡ï¸"

# ì‹¤ì œ ìƒí™© ì˜ˆì‹œ
print(adaptive_cruise_control(80, 30, 100))  # ì¶œë ¥: ê°ì† ğŸ”½
print(adaptive_cruise_control(80, 70, 100))  # ì¶œë ¥: ê°€ì† ğŸ”¼
```

### 2. ğŸ›£ï¸ LKA (Lane Keeping Assist) - ì°¨ì„  ìœ ì§€ ë³´ì¡°

**ì‘ë™ ì›ë¦¬**: ì°¨ì„  ì´íƒˆ ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ ì¡°í–¥ ë³´ì •

```python
class LaneKeepingAssist:
    def __init__(self):
        self.camera = "ì „ë°© ì¹´ë©”ë¼"
        self.warning_threshold = 0.3  # 30cm
        self.correction_threshold = 0.5  # 50cm
    
    def detect_lane_departure(self, image):
        """ì°¨ì„  ì´íƒˆ ê°ì§€"""
        # ì‹¤ì œë¡œëŠ” ì»´í“¨í„° ë¹„ì „ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
        left_line, right_line = self.detect_lane_lines(image)
        car_center = self.get_car_center()
        
        # ì°¨ì„  ì¤‘ì•™ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ ê³„ì‚°
        offset = self.calculate_offset(car_center, left_line, right_line)
        
        if abs(offset) > self.correction_threshold:
            return "ì¡°í–¥ ë³´ì • í•„ìš”", offset
        elif abs(offset) > self.warning_threshold:
            return "ê²½ê³ ", offset
        else:
            return "ì •ìƒ", offset
    
    def detect_lane_lines(self, image):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ ì°¨ì„  ê²€ì¶œ"""
        # PyTorch ëª¨ë¸ ì‚¬ìš©
        lanes = self.lane_detection_model(image)
        return lanes['left'], lanes['right']
```

### 3. ğŸš¨ AEB (Autonomous Emergency Braking) - ìë™ ê¸´ê¸‰ ì œë™

**ì‘ë™ ì›ë¦¬**: ì¶©ëŒ ìœ„í—˜ ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ ë¸Œë ˆì´í¬ ì‘ë™

```python
class AutoEmergencyBraking:
    def __init__(self):
        self.radar = "ì „ë°© ë ˆì´ë”"
        self.camera = "ì „ë°© ì¹´ë©”ë¼"
        
    def calculate_time_to_collision(self, distance, relative_speed):
        """ì¶©ëŒê¹Œì§€ ë‚¨ì€ ì‹œê°„ ê³„ì‚°"""
        if relative_speed <= 0:
            return float('inf')  # ì¶©ëŒ ìœ„í—˜ ì—†ìŒ
        
        ttc = distance / relative_speed
        return ttc
    
    def decide_action(self, ttc):
        """TTCì— ë”°ë¥¸ í–‰ë™ ê²°ì •"""
        if ttc < 0.6:
            return "ğŸš¨ ê¸´ê¸‰ ì œë™! (100% ë¸Œë ˆì´í¬)"
        elif ttc < 1.5:
            return "âš ï¸ ë¶€ë¶„ ì œë™ (50% ë¸Œë ˆì´í¬)"
        elif ttc < 3.0:
            return "ğŸ“¢ ê²½ê³ ìŒ ë°œìƒ"
        else:
            return "âœ… ì•ˆì „"
    
    def process(self, sensor_data):
        """AEB ì²˜ë¦¬ ê³¼ì •"""
        # 1. ì¥ì• ë¬¼ ê°ì§€
        obstacle = sensor_data['obstacle']
        distance = obstacle['distance']
        relative_speed = obstacle['relative_speed']
        
        # 2. TTC ê³„ì‚°
        ttc = self.calculate_time_to_collision(distance, relative_speed)
        
        # 3. í–‰ë™ ê²°ì •
        action = self.decide_action(ttc)
        
        print(f"ê±°ë¦¬: {distance}m, ìƒëŒ€ì†ë„: {relative_speed}m/s")
        print(f"ì¶©ëŒê¹Œì§€: {ttc:.1f}ì´ˆ")
        print(f"ì¡°ì¹˜: {action}")
        
        return action

# ì‹œë®¬ë ˆì´ì…˜
aeb = AutoEmergencyBraking()
sensor_data = {
    'obstacle': {
        'distance': 10,  # 10ë¯¸í„° ì•
        'relative_speed': 20  # 20m/së¡œ ì ‘ê·¼ ì¤‘
    }
}
aeb.process(sensor_data)
# ì¶œë ¥: ì¶©ëŒê¹Œì§€: 0.5ì´ˆ, ì¡°ì¹˜: ğŸš¨ ê¸´ê¸‰ ì œë™!
```

### 4. ğŸ“ BSD (Blind Spot Detection) - ì‚¬ê°ì§€ëŒ€ ê°ì§€

```python
class BlindSpotDetection:
    def __init__(self):
        self.side_radars = ["ì¢Œì¸¡ ë ˆì´ë”", "ìš°ì¸¡ ë ˆì´ë”"]
        self.warning_zones = {
            'left': {'x': (-4, -1), 'y': (-2, 0)},   # ì¢Œì¸¡ ì‚¬ê°ì§€ëŒ€
            'right': {'x': (-4, -1), 'y': (0, 2)}    # ìš°ì¸¡ ì‚¬ê°ì§€ëŒ€
        }
    
    def check_blind_spots(self, radar_data):
        """ì‚¬ê°ì§€ëŒ€ ì°¨ëŸ‰ í™•ì¸"""
        warnings = []
        
        for side in ['left', 'right']:
            if self.is_vehicle_in_zone(radar_data[side], self.warning_zones[side]):
                warnings.append(f"{side} ì‚¬ê°ì§€ëŒ€ ì°¨ëŸ‰ ê°ì§€! âš ï¸")
                # ì‚¬ì´ë“œë¯¸ëŸ¬ LED ê²½ê³ ë“± ì¼œê¸°
                self.activate_mirror_warning(side)
        
        return warnings
```

### 5. ğŸ…¿ï¸ APA (Automatic Parking Assist) - ìë™ ì£¼ì°¨ ë³´ì¡°

```python
class AutoParkingAssist:
    def __init__(self):
        self.ultrasonic_sensors = 12  # ì „ë°©ìœ„ ì´ˆìŒíŒŒ ì„¼ì„œ
        self.min_space_parallel = 6.0  # í‰í–‰ì£¼ì°¨ ìµœì†Œ ê³µê°„ (ë¯¸í„°)
        self.min_space_perpendicular = 2.5  # ì§ê°ì£¼ì°¨ ìµœì†Œ ê³µê°„
    
    def find_parking_space(self, sensor_data):
        """ì£¼ì°¨ ê³µê°„ íƒìƒ‰"""
        spaces = []
        
        for i, reading in enumerate(sensor_data):
            if reading['distance'] > self.min_space_parallel:
                spaces.append({
                    'type': 'parallel',
                    'size': reading['distance'],
                    'position': i
                })
        
        return spaces
    
    def execute_parking(self, space_info):
        """ìë™ ì£¼ì°¨ ì‹¤í–‰"""
        steps = []
        
        if space_info['type'] == 'parallel':
            steps = [
                "1. ì£¼ì°¨ê³µê°„ ì˜† ì •ë ¬ ğŸš—",
                "2. í›„ì§„ ê¸°ì–´ (R) ğŸ”„",
                "3. í•¸ë“¤ ìš°ì¸¡ ìµœëŒ€ â†ªï¸",
                "4. 45ë„ ê°ë„ê¹Œì§€ í›„ì§„ ğŸ“",
                "5. í•¸ë“¤ ë°˜ëŒ€ë¡œ â†©ï¸",
                "6. ì •ë ¬ ì™„ë£Œê¹Œì§€ í›„ì§„ âœ…",
                "7. ìœ„ì¹˜ ë¯¸ì„¸ì¡°ì • ğŸ¯"
            ]
        
        for step in steps:
            print(step)
            # ì‹¤ì œë¡œëŠ” ê° ë‹¨ê³„ë³„ ì°¨ëŸ‰ ì œì–´ ëª…ë ¹ ì‹¤í–‰
        
        return "ì£¼ì°¨ ì™„ë£Œ! ğŸ‰"
```

---

## ğŸ”§ ADAS ëª¨ë¸ ê°œë°œ íë¦„

### ğŸ“Š ì „ì²´ ê°œë°œ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    A[ë°ì´í„° ìˆ˜ì§‘] --> B[ë°ì´í„° ì „ì²˜ë¦¬]
    B --> C[ëª¨ë¸ í•™ìŠµ<br/>PyTorch]
    C --> D[ëª¨ë¸ ê²€ì¦]
    D --> E[ëª¨ë¸ ìµœì í™”<br/>TensorRT]
    E --> F[ì°¨ëŸ‰ ë°°í¬]
    F --> G[ì‹¤ì‹œê°„ ì¶”ë¡ ]
```

### 1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘ ë° ë¼ë²¨ë§

```python
import torch
import torchvision
from torch.utils.data import Dataset, DataLoader
import cv2
import json

class ADASDataset(Dataset):
    """ADAS í•™ìŠµìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, data_dir, annotations_file, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        
        # ì£¼ì„ íŒŒì¼ ë¡œë“œ (COCO í˜•ì‹)
        with open(annotations_file, 'r') as f:
            self.annotations = json.load(f)
        
        self.classes = {
            0: 'background',
            1: 'car',
            2: 'pedestrian',
            3: 'cyclist',
            4: 'traffic_light',
            5: 'traffic_sign',
            6: 'lane_line'
        }
    
    def __len__(self):
        return len(self.annotations['images'])
    
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_info = self.annotations['images'][idx]
        image_path = f"{self.data_dir}/{img_info['file_name']}"
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
        annotations = [ann for ann in self.annotations['annotations'] 
                      if ann['image_id'] == img_info['id']]
        
        # ë°”ìš´ë”© ë°•ìŠ¤ì™€ í´ë˜ìŠ¤ ì¶”ì¶œ
        boxes = []
        labels = []
        
        for ann in annotations:
            boxes.append(ann['bbox'])  # [x, y, width, height]
            labels.append(ann['category_id'])
        
        # í…ì„œë¡œ ë³€í™˜
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        
        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([idx])
        }
        
        if self.transform:
            image = self.transform(image)
        
        return image, target

# ë°ì´í„° ë¡œë” ìƒì„±
def create_data_loaders(batch_size=8):
    """í•™ìŠµìš© ë°ì´í„° ë¡œë” ìƒì„±"""
    
    transform = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = ADASDataset(
        data_dir='data/train',
        annotations_file='data/train_annotations.json',
        transform=transform
    )
    
    val_dataset = ADASDataset(
        data_dir='data/val',
        annotations_file='data/val_annotations.json',
        transform=transform
    )
    
    # ë°ì´í„° ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=lambda x: tuple(zip(*x))
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        collate_fn=lambda x: tuple(zip(*x))
    )
    
    return train_loader, val_loader
```

### 2ï¸âƒ£ PyTorch ëª¨ë¸ í•™ìŠµ

```python
import torch
import torch.nn as nn
import torchvision.models as models
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

class ADASObjectDetector:
    """ADASìš© ê°ì²´ ê²€ì¶œ ëª¨ë¸"""
    
    def __init__(self, num_classes=7):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_classes = num_classes
        self.model = self.create_model()
        
    def create_model(self):
        """Faster R-CNN ëª¨ë¸ ìƒì„±"""
        # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
        model = fasterrcnn_resnet50_fpn(pretrained=True)
        
        # ë¶„ë¥˜ê¸° í—¤ë“œ êµì²´
        in_features = model.roi_heads.box_predictor.cls_score.in_features
        model.roi_heads.box_predictor = FastRCNNPredictor(
            in_features, 
            self.num_classes
        )
        
        return model.to(self.device)
    
    def train_epoch(self, data_loader, optimizer):
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        
        for batch_idx, (images, targets) in enumerate(data_loader):
            # GPUë¡œ ì´ë™
            images = [img.to(self.device) for img in images]
            targets = [{k: v.to(self.device) for k, v in t.items()} 
                      for t in targets]
            
            # Forward pass
            loss_dict = self.model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            
            # Backward pass
            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
            
            total_loss += losses.item()
            
            if batch_idx % 10 == 0:
                print(f"Batch [{batch_idx}/{len(data_loader)}] "
                      f"Loss: {losses.item():.4f}")
        
        return total_loss / len(data_loader)
    
    def train(self, train_loader, val_loader, epochs=10):
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""
        optimizer = torch.optim.SGD(
            self.model.parameters(),
            lr=0.005,
            momentum=0.9,
            weight_decay=0.0005
        )
        
        lr_scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=3,
            gamma=0.1
        )
        
        best_loss = float('inf')
        
        for epoch in range(epochs):
            print(f"\nğŸ“š Epoch {epoch+1}/{epochs}")
            print("-" * 30)
            
            # í•™ìŠµ
            train_loss = self.train_epoch(train_loader, optimizer)
            print(f"Training Loss: {train_loss:.4f}")
            
            # ê²€ì¦
            val_loss = self.validate(val_loader)
            print(f"Validation Loss: {val_loss:.4f}")
            
            # í•™ìŠµë¥  ì¡°ì •
            lr_scheduler.step()
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if val_loss < best_loss:
                best_loss = val_loss
                self.save_model(f"best_adas_model_epoch_{epoch+1}.pth")
                print("âœ… Best model saved!")
    
    def validate(self, data_loader):
        """ëª¨ë¸ ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for images, targets in data_loader:
                images = [img.to(self.device) for img in images]
                targets = [{k: v.to(self.device) for k, v in t.items()} 
                          for t in targets]
                
                # Validation ëª¨ë“œì—ì„œëŠ” loss ê³„ì‚°ì„ ìœ„í•´ train ëª¨ë“œ í•„ìš”
                self.model.train()
                loss_dict = self.model(images, targets)
                losses = sum(loss for loss in loss_dict.values())
                self.model.eval()
                
                total_loss += losses.item()
        
        return total_loss / len(data_loader)
    
    def save_model(self, path):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'num_classes': self.num_classes
        }, path)
        print(f"Model saved to {path}")
    
    def load_model(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"Model loaded from {path}")

# í•™ìŠµ ì‹¤í–‰
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader, val_loader = create_data_loaders(batch_size=4)
    
    # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    detector = ADASObjectDetector(num_classes=7)
    detector.train(train_loader, val_loader, epochs=10)
```

---

## ğŸ”¥ TensorRT vs PyTorch ì™„ë²½ ë¹„êµ

### ğŸ“Š í•µì‹¬ ì°¨ì´ì  ìš”ì•½

| êµ¬ë¶„ | PyTorch | TensorRT |
|------|---------|----------|
| **ì£¼ìš” ëª©ì ** | ğŸ“ ì—°êµ¬ & ê°œë°œ | ğŸš€ ë°°í¬ & ì¶”ë¡  |
| **ì‚¬ìš© ë‹¨ê³„** | í•™ìŠµ & ì‹¤í—˜ | ìµœì í™” & ì‹¤í–‰ |
| **ìœ ì—°ì„±** | â­â­â­â­â­ ë§¤ìš° ë†’ìŒ | â­â­ ì œí•œì  |
| **ì„±ëŠ¥** | â­â­â­ ë³´í†µ | â­â­â­â­â­ ë§¤ìš° ë¹ ë¦„ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ë§ìŒ | ì ìŒ |
| **ì§€ì› í•˜ë“œì›¨ì–´** | CPU, GPU, TPU | NVIDIA GPU ì „ìš© |
| **í•™ìŠµ ê³¡ì„ ** | ì™„ë§Œí•¨ | ê°€íŒŒë¦„ |

### ğŸ¯ ìƒì„¸ ë¹„êµ

#### 1. ê°œë°œ í¸ì˜ì„±

```python
# PyTorch - ì§ê´€ì ì´ê³  ì‰¬ì›€
import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3)
        self.fc1 = nn.Linear(32 * 30 * 30, 10)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x

# ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
model = SimpleCNN()
output = model(torch.randn(1, 3, 32, 32))
```

```python
# TensorRT - ë³µì¡í•˜ì§€ë§Œ ë¹ ë¦„
import tensorrt as trt

def build_engine():
    """TensorRT ì—”ì§„ ë¹Œë“œ - ë” ë³µì¡"""
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network(
        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    )
    
    # ë„¤íŠ¸ì›Œí¬ ì •ì˜ (ìˆ˜ë™)
    input_tensor = network.add_input(
        name="input",
        dtype=trt.float32,
        shape=(1, 3, 32, 32)
    )
    
    # ë ˆì´ì–´ ì¶”ê°€
    conv1 = network.add_convolution(
        input=input_tensor,
        num_output_maps=32,
        kernel_shape=(3, 3),
        kernel=kernel_weights,
        bias=bias_weights
    )
    
    # ... ë” ë§ì€ ì„¤ì • í•„ìš”
    
    return builder.build_engine(network, config)
```

#### 2. ì„±ëŠ¥ ì°¨ì´

```python
import time

def benchmark_comparison():
    """PyTorch vs TensorRT ì„±ëŠ¥ ë¹„êµ"""
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 1
    input_shape = (batch_size, 3, 224, 224)
    num_iterations = 1000
    
    # PyTorch ì¶”ë¡ 
    pytorch_model = load_pytorch_model()
    pytorch_input = torch.randn(input_shape).cuda()
    
    torch.cuda.synchronize()
    start = time.time()
    
    with torch.no_grad():
        for _ in range(num_iterations):
            _ = pytorch_model(pytorch_input)
    
    torch.cuda.synchronize()
    pytorch_time = time.time() - start
    
    # TensorRT ì¶”ë¡ 
    trt_engine = load_trt_engine()
    trt_input = cuda.mem_alloc(input_size)
    trt_output = cuda.mem_alloc(output_size)
    
    cuda.memcpy_htod(trt_input, input_data)
    
    start = time.time()
    
    for _ in range(num_iterations):
        context.execute_v2([trt_input, trt_output])
    
    cuda.synchronize()
    trt_time = time.time() - start
    
    # ê²°ê³¼ ë¹„êµ
    print(f"ğŸ“Š ì„±ëŠ¥ ë¹„êµ (1000íšŒ ì¶”ë¡ )")
    print(f"PyTorch: {pytorch_time:.2f}ì´ˆ ({1000/pytorch_time:.1f} FPS)")
    print(f"TensorRT: {trt_time:.2f}ì´ˆ ({1000/trt_time:.1f} FPS)")
    print(f"ì†ë„ í–¥ìƒ: {pytorch_time/trt_time:.1f}ë°°")
    
    # ì¼ë°˜ì ì¸ ê²°ê³¼:
    # PyTorch: 10.5ì´ˆ (95 FPS)
    # TensorRT: 2.1ì´ˆ (476 FPS)
    # ì†ë„ í–¥ìƒ: 5.0ë°°
```

#### 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ëª¨ë¸ í¬ê¸° | PyTorch | TensorRT | ì ˆê°ìœ¨ |
|----------|---------|----------|--------|
| ResNet-50 | 98 MB | 45 MB | 54% |
| YOLOv5 | 165 MB | 62 MB | 62% |
| EfficientNet | 82 MB | 31 MB | 62% |

#### 4. ë°°í¬ í™˜ê²½

```python
# PyTorch - ë‹¤ì–‘í•œ í™˜ê²½ ì§€ì›
platforms = {
    "ì„œë²„": "âœ… Linux/Windows/Mac",
    "ëª¨ë°”ì¼": "âœ… iOS/Android (PyTorch Mobile)",
    "ì—£ì§€": "âœ… Raspberry Pi, Jetson",
    "ì›¹": "âœ… ONNX.js, TorchScript"
}

# TensorRT - NVIDIA ì „ìš©
platforms = {
    "ì„œë²„": "âœ… NVIDIA GPU ì„œë²„",
    "ëª¨ë°”ì¼": "âŒ ë¯¸ì§€ì›",
    "ì—£ì§€": "âœ… NVIDIA Jetsonë§Œ",
    "ì›¹": "âŒ ë¯¸ì§€ì›"
}
```

---

## ğŸ”„ ì‹¤ìŠµ: PyTorchì—ì„œ TensorRTë¡œ

### ğŸ¯ ì „ì²´ ë³€í™˜ ê³¼ì •

```mermaid
graph LR
    A[PyTorch ëª¨ë¸] --> B[ONNX ë³€í™˜]
    B --> C[TensorRT íŒŒì‹±]
    C --> D[ìµœì í™”]
    D --> E[TensorRT ì—”ì§„]
    E --> F[ë°°í¬]
```

### Step 1: PyTorch ëª¨ë¸ ì¤€ë¹„

```python
import torch
import torch.nn as nn
import torchvision.models as models

class LaneDetectionModel(nn.Module):
    """ì°¨ì„  ê²€ì¶œ ëª¨ë¸ (ADASìš©)"""
    
    def __init__(self, num_classes=2):  # ì°¨ì„ , ë°°ê²½
        super().__init__()
        # ë°±ë³¸: MobileNetV2 (ê²½ëŸ‰í™”)
        self.backbone = models.mobilenet_v2(pretrained=True).features
        
        # ë””ì½”ë” (ì„¸ê·¸ë©˜í…Œì´ì…˜)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(1280, 256, 3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(32, num_classes, 3, stride=2, padding=1)
        )
    
    def forward(self, x):
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ ìƒì„±
        output = self.decoder(features)
        return output

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
def prepare_pytorch_model():
    """PyTorch ëª¨ë¸ ì¤€ë¹„"""
    model = LaneDetectionModel(num_classes=2)
    model.load_state_dict(torch.load('lane_detection_model.pth'))
    model.eval()
    model.cuda()
    
    print("âœ… PyTorch ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
    return model

model = prepare_pytorch_model()

### Step 2: ONNXë¡œ ë³€í™˜

```python
import torch.onnx

def pytorch_to_onnx(model, onnx_path='lane_detection.onnx'):
    """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜"""
    
    # ë”ë¯¸ ì…ë ¥ (ë°°ì¹˜í¬ê¸° 1, ì±„ë„ 3, ë†’ì´ 224, ë„ˆë¹„ 224)
    dummy_input = torch.randn(1, 3, 224, 224).cuda()
    
    # ONNX ë‚´ë³´ë‚´ê¸°
    torch.onnx.export(
        model,                      # ëª¨ë¸
        dummy_input,                # ì…ë ¥ ì˜ˆì‹œ
        onnx_path,                  # ì €ì¥ ê²½ë¡œ
        export_params=True,         # í•™ìŠµëœ íŒŒë¼ë¯¸í„° í¬í•¨
        opset_version=11,           # ONNX ë²„ì „
        do_constant_folding=True,   # ìƒìˆ˜ í´ë”© ìµœì í™”
        input_names=['input'],      # ì…ë ¥ ì´ë¦„
        output_names=['output'],    # ì¶œë ¥ ì´ë¦„
        dynamic_axes={              # ë™ì  ì¶• (ë°°ì¹˜ í¬ê¸°)
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    print(f"âœ… ONNX ë³€í™˜ ì™„ë£Œ: {onnx_path}")
    
    # ONNX ëª¨ë¸ ê²€ì¦
    import onnx
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print("âœ… ONNX ëª¨ë¸ ê²€ì¦ ì„±ê³µ!")
    
    return onnx_path

onnx_path = pytorch_to_onnx(model)
```

### Step 3: TensorRT ì—”ì§„ ìƒì„±

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

class TensorRTConverter:
    """ONNXë¥¼ TensorRTë¡œ ë³€í™˜"""
    
    def __init__(self, onnx_path, precision='FP16'):
        """
        precision: 'FP32', 'FP16', 'INT8'
        """
        self.onnx_path = onnx_path
        self.precision = precision
        self.logger = trt.Logger(trt.Logger.WARNING)
        
    def build_engine(self, engine_path='lane_detection.trt'):
        """TensorRT ì—”ì§„ ë¹Œë“œ"""
        
        print(f"ğŸ”§ TensorRT ì—”ì§„ ë¹Œë“œ ì‹œì‘ (ì •ë°€ë„: {self.precision})")
        
        # ë¹Œë” ìƒì„±
        builder = trt.Builder(self.logger)
        network = builder.create_network(
            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        )
        parser = trt.OnnxParser(network, self.logger)
        
        # ONNX íŒŒì¼ íŒŒì‹±
        with open(self.onnx_path, 'rb') as f:
            if not parser.parse(f.read()):
                print("âŒ ONNX íŒŒì‹± ì‹¤íŒ¨!")
                for error in range(parser.num_errors):
                    print(parser.get_error(error))
                return None
        
        print("âœ… ONNX íŒŒì‹± ì„±ê³µ!")
        
        # ë¹Œë” ì„¤ì •
        config = builder.create_builder_config()
        config.max_workspace_size = 1 << 30  # 1GB
        
        # ì •ë°€ë„ ì„¤ì •
        if self.precision == 'FP16':
            config.set_flag(trt.BuilderFlag.FP16)
        elif self.precision == 'INT8':
            config.set_flag(trt.BuilderFlag.INT8)
            # INT8 ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš” (ìƒëµ)
        
        # ìµœì í™” í”„ë¡œíŒŒì¼ ì„¤ì • (ë™ì  ë°°ì¹˜ í¬ê¸°)
        profile = builder.create_optimization_profile()
        profile.set_shape(
            'input',
            (1, 3, 224, 224),    # ìµœì†Œ
            (4, 3, 224, 224),    # ìµœì 
            (8, 3, 224, 224)     # ìµœëŒ€
        )
        config.add_optimization_profile(profile)
        
        # ì—”ì§„ ë¹Œë“œ
        print("ğŸ—ï¸ ì—”ì§„ ë¹Œë“œ ì¤‘... (ëª‡ ë¶„ ì†Œìš”)")
        engine = builder.build_engine(network, config)
        
        if engine is None:
            print("âŒ ì—”ì§„ ë¹Œë“œ ì‹¤íŒ¨!")
            return None
        
        # ì—”ì§„ ì €ì¥
        with open(engine_path, 'wb') as f:
            f.write(engine.serialize())
        
        print(f"âœ… TensorRT ì—”ì§„ ì €ì¥ ì™„ë£Œ: {engine_path}")
        return engine_path
    
    def profile_engine(self, engine_path):
        """ì—”ì§„ í”„ë¡œíŒŒì¼ë§"""
        with open(engine_path, 'rb') as f:
            runtime = trt.Runtime(self.logger)
            engine = runtime.deserialize_cuda_engine(f.read())
        
        print("\nğŸ“Š TensorRT ì—”ì§„ ì •ë³´:")
        print(f"- ë ˆì´ì–´ ìˆ˜: {engine.num_layers}")
        print(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {engine.device_memory_size / 1024**2:.2f} MB")
        print(f"- ìµœëŒ€ ë°°ì¹˜ í¬ê¸°: {engine.max_batch_size}")
        
        # ì…ì¶œë ¥ ì •ë³´
        for i in range(engine.num_bindings):
            name = engine.get_binding_name(i)
            shape = engine.get_binding_shape(i)
            dtype = engine.get_binding_dtype(i)
            
            if engine.binding_is_input(i):
                print(f"- ì…ë ¥: {name}, í˜•íƒœ: {shape}, íƒ€ì…: {dtype}")
            else:
                print(f"- ì¶œë ¥: {name}, í˜•íƒœ: {shape}, íƒ€ì…: {dtype}")

# TensorRT ë³€í™˜ ì‹¤í–‰
converter = TensorRTConverter(onnx_path, precision='FP16')
trt_engine_path = converter.build_engine()
converter.profile_engine(trt_engine_path)
```

### Step 4: TensorRT ì¶”ë¡  ì‹¤í–‰

```python
import numpy as np
import cv2

class TensorRTInference:
    """TensorRT ì¶”ë¡  í´ë˜ìŠ¤"""
    
    def __init__(self, engine_path):
        self.logger = trt.Logger(trt.Logger.WARNING)
        
        # ì—”ì§„ ë¡œë“œ
        with open(engine_path, 'rb') as f:
            self.runtime = trt.Runtime(self.logger)
            self.engine = self.runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
        
        # ë²„í¼ í• ë‹¹
        self.allocate_buffers()
        
        print("âœ… TensorRT ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ!")
    
    def allocate_buffers(self):
        """GPU ë©”ëª¨ë¦¬ ë²„í¼ í• ë‹¹"""
        self.inputs = []
        self.outputs = []
        self.bindings = []
        self.stream = cuda.Stream()
        
        for binding in self.engine:
            shape = self.engine.get_binding_shape(binding)
            size = trt.volume(shape)
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))
            
            # í˜¸ìŠ¤íŠ¸ ë° ë””ë°”ì´ìŠ¤ ë©”ëª¨ë¦¬ í• ë‹¹
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            
            self.bindings.append(int(device_mem))
            
            if self.engine.binding_is_input(binding):
                self.inputs.append({
                    'host': host_mem,
                    'device': device_mem,
                    'shape': shape
                })
            else:
                self.outputs.append({
                    'host': host_mem,
                    'device': device_mem,
                    'shape': shape
                })
    
    def preprocess(self, image_path):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        image = cv2.resize(image, (224, 224))
        
        # ì •ê·œí™”
        image = image.astype(np.float32) / 255.0
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = (image - mean) / std
        
        # ì°¨ì› ë³€ê²½: HWC -> CHW
        image = np.transpose(image, (2, 0, 1))
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        image = np.expand_dims(image, axis=0)
        
        return image
    
    def infer(self, image_path):
        """ì¶”ë¡  ì‹¤í–‰"""
        # ì „ì²˜ë¦¬
        input_data = self.preprocess(image_path)
        
        # ì…ë ¥ ë°ì´í„° ë³µì‚¬
        np.copyto(self.inputs[0]['host'], input_data.ravel())
        
        # H2D ì „ì†¡
        cuda.memcpy_htod_async(
            self.inputs[0]['device'],
            self.inputs[0]['host'],
            self.stream
        )
        
        # ì¶”ë¡  ì‹¤í–‰
        self.context.execute_async_v2(
            bindings=self.bindings,
            stream_handle=self.stream.handle
        )
        
        # D2H ì „ì†¡
        cuda.memcpy_dtoh_async(
            self.outputs[0]['host'],
            self.outputs[0]['device'],
            self.stream
        )
        
        # ë™ê¸°í™”
        self.stream.synchronize()
        
        # ê²°ê³¼ ì¬êµ¬ì„±
        output = self.outputs[0]['host'].reshape(self.outputs[0]['shape'])
        
        return output
    
    def postprocess(self, output):
        """í›„ì²˜ë¦¬: ì°¨ì„  ë§ˆìŠ¤í¬ ìƒì„±"""
        # Softmax (2í´ë˜ìŠ¤: ë°°ê²½, ì°¨ì„ )
        output = np.exp(output) / np.sum(np.exp(output), axis=1, keepdims=True)
        
        # ì°¨ì„  í´ë˜ìŠ¤ í™•ë¥ 
        lane_prob = output[0, 1, :, :]  # ë°°ì¹˜ 0, í´ë˜ìŠ¤ 1 (ì°¨ì„ )
        
        # ì„ê³„ê°’ ì ìš©
        lane_mask = (lane_prob > 0.5).astype(np.uint8) * 255
        
        return lane_mask
    
    def visualize(self, image_path, lane_mask):
        """ê²°ê³¼ ì‹œê°í™”"""
        # ì›ë³¸ ì´ë¯¸ì§€
        original = cv2.imread(image_path)
        original = cv2.resize(original, (224, 224))
        
        # ì°¨ì„  ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
        colored_mask = np.zeros_like(original)
        colored_mask[:, :, 1] = lane_mask  # ë…¹ìƒ‰ìœ¼ë¡œ í‘œì‹œ
        
        # í•©ì„±
        result = cv2.addWeighted(original, 0.7, colored_mask, 0.3, 0)
        
        return result

# TensorRT ì¶”ë¡  ì‹¤í–‰
def test_tensorrt_inference():
    """TensorRT ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    
    # ì¶”ë¡ ê¸° ìƒì„±
    inferencer = TensorRTInference('lane_detection.trt')
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤
    test_images = [
        'test_images/highway.jpg',
        'test_images/city_road.jpg',
        'test_images/curved_lane.jpg'
    ]
    
    # ì¶”ë¡  ì‹¤í–‰
    for img_path in test_images:
        print(f"\nğŸ–¼ï¸ ì²˜ë¦¬ ì¤‘: {img_path}")
        
        # ì¶”ë¡ 
        output = inferencer.infer(img_path)
        
        # í›„ì²˜ë¦¬
        lane_mask = inferencer.postprocess(output)
        
        # ì‹œê°í™”
        result = inferencer.visualize(img_path, lane_mask)
        
        # ì €ì¥
        output_path = img_path.replace('.jpg', '_result.jpg')
        cv2.imwrite(output_path, result)
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_path}")

# ì‹¤í–‰
test_tensorrt_inference()
```

### Step 5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
import time

def benchmark_comparison():
    """PyTorch vs TensorRT ì„±ëŠ¥ ë¹„êµ"""
    
    print("\n" + "="*50)
    print("ğŸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬: PyTorch vs TensorRT")
    print("="*50)
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    num_iterations = 100
    image_path = 'test_images/highway.jpg'
    
    # 1. PyTorch ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ“Š PyTorch ì¶”ë¡ ...")
    pytorch_model = prepare_pytorch_model()
    
    # ì›Œë°ì—…
    for _ in range(10):
        dummy_input = torch.randn(1, 3, 224, 224).cuda()
        with torch.no_grad():
            _ = pytorch_model(dummy_input)
    
    # ì‹¤ì œ ì¸¡ì •
    torch.cuda.synchronize()
    start_time = time.time()
    
    for _ in range(num_iterations):
        dummy_input = torch.randn(1, 3, 224, 224).cuda()
        with torch.no_grad():
            _ = pytorch_model(dummy_input)
    
    torch.cuda.synchronize()
    pytorch_time = time.time() - start_time
    pytorch_fps = num_iterations / pytorch_time
    
    # 2. TensorRT ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ“Š TensorRT ì¶”ë¡ ...")
    trt_inferencer = TensorRTInference('lane_detection.trt')
    
    # ì›Œë°ì—…
    for _ in range(10):
        _ = trt_inferencer.infer(image_path)
    
    # ì‹¤ì œ ì¸¡ì •
    start_time = time.time()
    
    for _ in range(num_iterations):
        _ = trt_inferencer.infer(image_path)
    
    trt_time = time.time() - start_time
    trt_fps = num_iterations / trt_time
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
    print("="*50)
    
    results = f"""
    | í•­ëª© | PyTorch | TensorRT | ê°œì„  |
    |------|---------|----------|------|
    | ì´ ì‹œê°„ | {pytorch_time:.2f}ì´ˆ | {trt_time:.2f}ì´ˆ | {pytorch_time/trt_time:.1f}x |
    | FPS | {pytorch_fps:.1f} | {trt_fps:.1f} | {trt_fps/pytorch_fps:.1f}x |
    | ì§€ì—°ì‹œê°„ | {1000/pytorch_fps:.2f}ms | {1000/trt_fps:.2f}ms | {(1000/pytorch_fps)/(1000/trt_fps):.1f}x |
    """
    
    print(results)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    print(f"PyTorch ëª¨ë¸: ~98 MB")
    print(f"TensorRT ì—”ì§„: ~35 MB")
    print(f"ë©”ëª¨ë¦¬ ì ˆê°: {(1 - 35/98)*100:.1f}%")
    
    # ADAS ìš”êµ¬ì‚¬í•­ ì²´í¬
    print("\nâœ… ADAS ì‹¤ì‹œê°„ ìš”êµ¬ì‚¬í•­ ì²´í¬:")
    required_fps = 30  # ì¼ë°˜ì ì¸ ADAS ìš”êµ¬ì‚¬í•­
    
    if pytorch_fps >= required_fps:
        print(f"PyTorch: âœ… ì¶©ì¡± ({pytorch_fps:.1f} >= {required_fps} FPS)")
    else:
        print(f"PyTorch: âŒ ë¯¸ì¶©ì¡± ({pytorch_fps:.1f} < {required_fps} FPS)")
    
    if trt_fps >= required_fps:
        print(f"TensorRT: âœ… ì¶©ì¡± ({trt_fps:.1f} >= {required_fps} FPS)")
    else:
        print(f"TensorRT: âŒ ë¯¸ì¶©ì¡± ({trt_fps:.1f} < {required_fps} FPS)")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_comparison()
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. TensorRT ìµœì í™” ê¸°ë²•

```python
class AdvancedTensorRTOptimization:
    """ê³ ê¸‰ TensorRT ìµœì í™” ê¸°ë²•"""
    
    def optimize_for_latency(self, builder_config):
        """ì§€ì—°ì‹œê°„ ìµœì í™”"""
        # ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰
        builder_config.max_workspace_size = 1 << 20  # ì‘ì€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤
        builder_config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)
        
        # ë ˆì´ì–´ í“¨ì „ ìµœëŒ€í™”
        builder_config.set_flag(trt.BuilderFlag.STRICT_TYPES)
        
        return builder_config
    
    def optimize_for_throughput(self, builder_config):
        """ì²˜ë¦¬ëŸ‰ ìµœì í™”"""
        # í° ì›Œí¬ìŠ¤í˜ì´ìŠ¤
        builder_config.max_workspace_size = 1 << 32  # 4GB
        
        # DLA í™œìš© (Jetson)
        if self.has_dla():
            builder_config.default_device_type = trt.DeviceType.DLA
            builder_config.DLA_core = 0
        
        return builder_config
    
    def int8_calibration(self, network, builder_config):
        """INT8 ìº˜ë¦¬ë¸Œë ˆì´ì…˜"""
        
        class Int8Calibrator(trt.IInt8EntropyCalibrator2):
            def __init__(self, data_loader):
                super().__init__()
                self.data_loader = data_loader
                self.current_index = 0
            
            def get_batch(self, names):
                if self.current_index < len(self.data_loader):
                    batch = self.data_loader[self.current_index]
                    self.current_index += 1
                    return [batch]
                return None
            
            def read_calibration_cache(self):
                # ìºì‹œ ì½ê¸°
                if os.path.exists('calibration.cache'):
                    with open('calibration.cache', 'rb') as f:
                        return f.read()
                return None
            
            def write_calibration_cache(self, cache):
                # ìºì‹œ ì €ì¥
                with open('calibration.cache', 'wb') as f:
                    f.write(cache)
        
        # INT8 ì„¤ì •
        builder_config.set_flag(trt.BuilderFlag.INT8)
        builder_config.int8_calibrator = Int8Calibrator(calibration_data)
        
        return builder_config
```

### 2. PyTorch ìµœì í™” ê¸°ë²•

```python
# 1. TorchScript ë³€í™˜
scripted_model = torch.jit.script(model)
scripted_model.save('model_scripted.pt')

# 2. ì–‘ìí™”
quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8
)

# 3. í”„ë£¨ë‹
import torch.nn.utils.prune as prune

prune.l1_unstructured(model.conv1, name='weight', amount=0.3)

# 4. ONNX Runtime ì‚¬ìš©
import onnxruntime as ort

session = ort.InferenceSession('model.onnx')
output = session.run(None, {'input': input_data})
```

---

## ğŸš€ ì‹¤ì œ ì°¨ëŸ‰ ë°°í¬ ê°€ì´ë“œ

### ğŸ“¦ ë°°í¬ í”Œë«í¼ë³„ ê°€ì´ë“œ

#### 1. NVIDIA Jetson (ì—£ì§€ ë””ë°”ì´ìŠ¤)

```bash
# Jetson ì„¤ì •
# 1. JetPack ì„¤ì¹˜ (TensorRT í¬í•¨)
sudo apt-get update
sudo apt-get install nvidia-jetpack

# 2. ì „ë ¥ ëª¨ë“œ ì„¤ì • (ìµœëŒ€ ì„±ëŠ¥)
sudo nvpmodel -m 0
sudo jetson_clocks

# 3. TensorRT ì—”ì§„ ìƒì„± (Jetson ìµœì í™”)
trtexec --onnx=model.onnx \
        --saveEngine=model_jetson.trt \
        --fp16 \
        --workspace=256 \
        --dla=0  # DLA ì½”ì–´ ì‚¬ìš©
```

#### 2. ì°¨ëŸ‰ ECU í†µí•©

```python
class VehicleADASIntegration:
    """ì‹¤ì œ ì°¨ëŸ‰ ADAS ì‹œìŠ¤í…œ í†µí•©"""
    
    def __init__(self):
        self.can_bus = CANBus()  # ì°¨ëŸ‰ CAN í†µì‹ 
        self.trt_engine = load_trt_engine()
        self.safety_monitor = SafetyMonitor()
    
    def process_frame(self, camera_frame):
        """ì‹¤ì‹œê°„ í”„ë ˆì„ ì²˜ë¦¬"""
        
        # 1. ì¶”ë¡ 
        detections = self.trt_engine.infer(camera_frame)
        
        # 2. ì•ˆì „ì„± ì²´í¬
        if not self.safety_monitor.validate(detections):
            return self.fallback_mode()
        
        # 3. CAN ë©”ì‹œì§€ ìƒì„±
        can_msg = self.create_can_message(detections)
        
        # 4. ì°¨ëŸ‰ ì œì–´ ì „ì†¡
        self.can_bus.send(can_msg)
        
        return detections
```

---

## ğŸ“š ìš©ì–´ì§‘

| ìš©ì–´ | ì˜ë¬¸ | ì„¤ëª… |
|------|------|------|
| **ADAS** | Advanced Driver Assistance Systems | ì²¨ë‹¨ ìš´ì „ì ë³´ì¡° ì‹œìŠ¤í…œ |
| **ACC** | Adaptive Cruise Control | ì•ì°¨ ê°„ê²© ìë™ ìœ ì§€ |
| **AEB** | Autonomous Emergency Braking | ìë™ ê¸´ê¸‰ ì œë™ |
| **LKA** | Lane Keeping Assist | ì°¨ì„  ìœ ì§€ ë³´ì¡° |
| **BSD** | Blind Spot Detection | ì‚¬ê°ì§€ëŒ€ ê°ì§€ |
| **TTC** | Time To Collision | ì¶©ëŒê¹Œì§€ ë‚¨ì€ ì‹œê°„ |
| **ì¶”ë¡ ** | Inference | í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì • |
| **ì–‘ìí™”** | Quantization | ëª¨ë¸ ì •ë°€ë„ë¥¼ ë‚®ì¶° ì†ë„ í–¥ìƒ |
| **í”„ë£¨ë‹** | Pruning | ë¶ˆí•„ìš”í•œ ê°€ì¤‘ì¹˜ ì œê±° |
| **ì§€ì—°ì‹œê°„** | Latency | ì…ë ¥ì—ì„œ ì¶œë ¥ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ |
| **ì²˜ë¦¬ëŸ‰** | Throughput | ë‹¨ìœ„ ì‹œê°„ë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°ì´í„°ì–‘ |
| **FPS** | Frames Per Second | ì´ˆë‹¹ ì²˜ë¦¬ í”„ë ˆì„ ìˆ˜ |
| **DLA** | Deep Learning Accelerator | NVIDIAì˜ ë”¥ëŸ¬ë‹ ì „ìš© í”„ë¡œì„¸ì„œ |
| **ECU** | Electronic Control Unit | ì°¨ëŸ‰ ì „ì ì œì–´ ì¥ì¹˜ |
| **CAN** | Controller Area Network | ì°¨ëŸ‰ ë‚´ë¶€ í†µì‹  í”„ë¡œí† ì½œ |

---



Â© 2025 ADAS Development Guide

</div>
